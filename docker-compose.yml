version: "3.9"

services:
  vllm-service:
    # 直接使用官方镜像，不用自己构建
    image: vllm/vllm-openai:latest
    container_name: my_vllm_container
    ports:
      - "8000:8000"
    volumes:
      - ./models:/models:ro
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - VLLM_LOGGING_LEVEL=INFO
      - HF_HUB_OFFLINE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]
    # command:
    #   [
    #     "--model", "/models/Qwen3-0.6B",
    #     "--served-model-name", "Qwen3-0.6B",
    #     "--gpu-memory-utilization", "0.75",
    #     "--max-model-len", "8000",
    #     "--max-num-seqs", "16",
    #     "--port", "8000"
    #   ]
    # command: # 加入quantization量化參數、縮小其他參數值(如下)，可以加大模型且實測成功
    #   [
    #     "--model", "/models/Qwen3-1.7B",
    #     "--served-model-name", "Qwen3-1.7B",
    #     "--gpu-memory-utilization", "0.6", # 使用 60% 的 GPU 記憶體
    #     "--max-model-len", "3500", # 最大上下文長度 3500 tokens
    #     "--max-num-seqs", "8", #同時處理的最大序列數
    #     "--quantization", "fp8", 
    #     "--port", "8000"
    #   ]
    command:
      [
        "--model", "/models/Qwen3-1.7B",
        "--served-model-name", "Qwen3-1.7B",
        "--gpu-memory-utilization", "0.8",
        "--max-model-len", "8000",
        "--max-num-seqs", "10",
        "--quantization", "fp8", 
        "--port", "8000"
      ]
  streamlit-app:
    # 内联 Dockerfile，不需要单独的文件
    build:
      context: .
      dockerfile_inline: |
        FROM python:3.10-slim
        RUN apt-get update && apt-get install -y gcc g++ curl && rm -rf /var/lib/apt/lists/*
        WORKDIR /app
        COPY requirements.txt .
        RUN pip install --no-cache-dir -r requirements.txt
        COPY . .
        EXPOSE 8501
        CMD ["streamlit", "run", "rag_chatroom_vllm.py", "--server.address", "0.0.0.0"]
    container_name: my_streamlit_app
    ports:
      - "8501:8501"
    volumes:
      - ./erp_data.json:/app/erp_data.json:ro
    environment:
      - VLLM_ENDPOINT=http://vllm-service:8000/v1
      # - LLM_MODEL=Qwen3-0.6B
      - LLM_MODEL=Qwen3-1.7B

    depends_on:
      - vllm-service